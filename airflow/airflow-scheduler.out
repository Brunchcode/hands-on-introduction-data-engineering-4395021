[[34m2024-04-23T23:44:06.451+0000[0m] {[34mscheduler_job_runner.py:[0m788} INFO[0m - Starting the scheduler[0m
[[34m2024-04-23T23:44:06.451+0000[0m] {[34mscheduler_job_runner.py:[0m795} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-04-23T23:44:06.456+0000[0m] {[34mmanager.py:[0m165} INFO[0m - Launched DagFileProcessorManager with pid: 6267[0m
[[34m2024-04-23T23:44:06.458+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-04-23T23:44:06.459+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2024-04-23T23:44:06.478+0000] {manager.py:411} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-04-23T23:45:27.535+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun basic_etl_dag @ 2024-04-23 23:45:26.138475+00:00: manual__2024-04-23T23:45:26.138475+00:00, state:running, queued_at: 2024-04-23 23:45:26.146922+00:00. externally triggered: True> successful[0m
[[34m2024-04-23T23:45:27.536+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=basic_etl_dag, execution_date=2024-04-23 23:45:26.138475+00:00, run_id=manual__2024-04-23T23:45:26.138475+00:00, run_start_date=2024-04-23 23:45:27.495040+00:00, run_end_date=2024-04-23 23:45:27.535965+00:00, run_duration=0.040925, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-04-23 23:45:26.138475+00:00, data_interval_end=2024-04-23 23:45:26.138475+00:00, dag_hash=0114bd36af12db9e8d9a78032944ac4c[0m
[[34m2024-04-23T23:49:06.606+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-04-23T23:50:50.060+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.extract_task manual__2024-04-23T23:50:48.169309+00:00 [scheduled]>[0m
[[34m2024-04-23T23:50:50.060+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2024-04-23T23:50:50.060+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.extract_task manual__2024-04-23T23:50:48.169309+00:00 [scheduled]>[0m
[[34m2024-04-23T23:50:50.062+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2024-04-23T23:50:48.169309+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-04-23T23:50:50.062+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2024-04-23T23:50:48.169309+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-23T23:50:50.091+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2024-04-23T23:50:48.169309+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-23T23:50:50.897+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2024-04-23T23:50:51.746+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: basic_etl_dag.extract_task manual__2024-04-23T23:50:48.169309+00:00 [queued]> on host codespaces-1d31e6[0m
[[34m2024-04-23T23:50:53.101+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2024-04-23T23:50:48.169309+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-23T23:50:53.107+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=extract_task, run_id=manual__2024-04-23T23:50:48.169309+00:00, map_index=-1, run_start_date=2024-04-23 23:50:51.817564+00:00, run_end_date=2024-04-23 23:50:52.690278+00:00, run_duration=0.872714, state=success, executor_state=success, try_number=1, max_tries=0, job_id=3, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-04-23 23:50:50.061314+00:00, queued_by_job_id=2, pid=9063[0m
[[34m2024-04-23T23:50:53.175+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.transform_task manual__2024-04-23T23:50:48.169309+00:00 [scheduled]>[0m
[[34m2024-04-23T23:50:53.175+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2024-04-23T23:50:53.176+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.transform_task manual__2024-04-23T23:50:48.169309+00:00 [scheduled]>[0m
[[34m2024-04-23T23:50:53.177+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2024-04-23T23:50:48.169309+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-04-23T23:50:53.177+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2024-04-23T23:50:48.169309+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-23T23:50:53.206+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2024-04-23T23:50:48.169309+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-23T23:50:53.969+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2024-04-23T23:50:54.796+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: basic_etl_dag.transform_task manual__2024-04-23T23:50:48.169309+00:00 [queued]> on host codespaces-1d31e6[0m
[[34m2024-04-23T23:50:55.509+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2024-04-23T23:50:48.169309+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-23T23:50:55.513+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=transform_task, run_id=manual__2024-04-23T23:50:48.169309+00:00, map_index=-1, run_start_date=2024-04-23 23:50:54.871747+00:00, run_end_date=2024-04-23 23:50:55.082491+00:00, run_duration=0.210744, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-04-23 23:50:53.176531+00:00, queued_by_job_id=2, pid=9082[0m
[[34m2024-04-23T23:50:55.574+0000[0m] {[34mdagrun.py:[0m609} ERROR[0m - Marking run <DagRun basic_etl_dag @ 2024-04-23 23:50:48.169309+00:00: manual__2024-04-23T23:50:48.169309+00:00, state:running, queued_at: 2024-04-23 23:50:48.181996+00:00. externally triggered: True> failed[0m
[[34m2024-04-23T23:50:55.574+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=basic_etl_dag, execution_date=2024-04-23 23:50:48.169309+00:00, run_id=manual__2024-04-23T23:50:48.169309+00:00, run_start_date=2024-04-23 23:50:49.981757+00:00, run_end_date=2024-04-23 23:50:55.574458+00:00, run_duration=5.592701, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-04-23 23:50:48.169309+00:00, data_interval_end=2024-04-23 23:50:48.169309+00:00, dag_hash=8a103ebeb8ed08310121a0fbe0f247aa[0m
[[34m2024-04-23T23:52:26.734+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.extract_task manual__2024-04-23T23:52:25.913407+00:00 [scheduled]>[0m
[[34m2024-04-23T23:52:26.734+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2024-04-23T23:52:26.734+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.extract_task manual__2024-04-23T23:52:25.913407+00:00 [scheduled]>[0m
[[34m2024-04-23T23:52:27.465+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2024-04-23T23:52:25.913407+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-04-23T23:52:27.465+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2024-04-23T23:52:25.913407+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-23T23:52:27.493+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2024-04-23T23:52:25.913407+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-23T23:52:28.290+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2024-04-23T23:52:29.138+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: basic_etl_dag.extract_task manual__2024-04-23T23:52:25.913407+00:00 [queued]> on host codespaces-1d31e6[0m
[[34m2024-04-23T23:52:30.596+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2024-04-23T23:52:25.913407+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-23T23:52:30.600+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=extract_task, run_id=manual__2024-04-23T23:52:25.913407+00:00, map_index=-1, run_start_date=2024-04-23 23:52:29.213122+00:00, run_end_date=2024-04-23 23:52:30.159960+00:00, run_duration=0.946838, state=success, executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-04-23 23:52:26.735151+00:00, queued_by_job_id=2, pid=9648[0m
[[34m2024-04-23T23:52:30.718+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.transform_task manual__2024-04-23T23:52:25.913407+00:00 [scheduled]>[0m
[[34m2024-04-23T23:52:30.719+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2024-04-23T23:52:30.719+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.transform_task manual__2024-04-23T23:52:25.913407+00:00 [scheduled]>[0m
[[34m2024-04-23T23:52:30.720+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2024-04-23T23:52:25.913407+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-04-23T23:52:30.720+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2024-04-23T23:52:25.913407+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-23T23:52:30.749+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2024-04-23T23:52:25.913407+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-23T23:52:31.518+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2024-04-23T23:52:32.373+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: basic_etl_dag.transform_task manual__2024-04-23T23:52:25.913407+00:00 [queued]> on host codespaces-1d31e6[0m
[[34m2024-04-23T23:52:33.033+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2024-04-23T23:52:25.913407+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-23T23:52:33.037+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=transform_task, run_id=manual__2024-04-23T23:52:25.913407+00:00, map_index=-1, run_start_date=2024-04-23 23:52:32.444003+00:00, run_end_date=2024-04-23 23:52:32.640487+00:00, run_duration=0.196484, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-04-23 23:52:30.719725+00:00, queued_by_job_id=2, pid=9685[0m
[[34m2024-04-23T23:52:33.067+0000[0m] {[34mdagrun.py:[0m609} ERROR[0m - Marking run <DagRun basic_etl_dag @ 2024-04-23 23:52:25.913407+00:00: manual__2024-04-23T23:52:25.913407+00:00, state:running, queued_at: 2024-04-23 23:52:25.918715+00:00. externally triggered: True> failed[0m
[[34m2024-04-23T23:52:33.067+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=basic_etl_dag, execution_date=2024-04-23 23:52:25.913407+00:00, run_id=manual__2024-04-23T23:52:25.913407+00:00, run_start_date=2024-04-23 23:52:26.629034+00:00, run_end_date=2024-04-23 23:52:33.067387+00:00, run_duration=6.438353, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-04-23 23:52:25.913407+00:00, data_interval_end=2024-04-23 23:52:25.913407+00:00, dag_hash=8a103ebeb8ed08310121a0fbe0f247aa[0m
[[34m2024-04-23T23:54:06.666+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-04-23T23:54:17.951+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.extract_task manual__2024-04-23T23:54:16.535831+00:00 [scheduled]>[0m
[[34m2024-04-23T23:54:17.951+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2024-04-23T23:54:17.952+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.extract_task manual__2024-04-23T23:54:16.535831+00:00 [scheduled]>[0m
[[34m2024-04-23T23:54:18.032+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2024-04-23T23:54:16.535831+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-04-23T23:54:18.032+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2024-04-23T23:54:16.535831+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-23T23:54:18.061+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2024-04-23T23:54:16.535831+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-23T23:54:18.861+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2024-04-23T23:54:19.695+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: basic_etl_dag.extract_task manual__2024-04-23T23:54:16.535831+00:00 [queued]> on host codespaces-1d31e6[0m
[[34m2024-04-23T23:54:21.028+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2024-04-23T23:54:16.535831+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-23T23:54:21.031+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=extract_task, run_id=manual__2024-04-23T23:54:16.535831+00:00, map_index=-1, run_start_date=2024-04-23 23:54:19.778769+00:00, run_end_date=2024-04-23 23:54:20.619477+00:00, run_duration=0.840708, state=success, executor_state=success, try_number=1, max_tries=0, job_id=7, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-04-23 23:54:17.952551+00:00, queued_by_job_id=2, pid=10371[0m
[[34m2024-04-23T23:54:21.125+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.transform_task manual__2024-04-23T23:54:16.535831+00:00 [scheduled]>[0m
[[34m2024-04-23T23:54:21.126+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2024-04-23T23:54:21.126+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.transform_task manual__2024-04-23T23:54:16.535831+00:00 [scheduled]>[0m
[[34m2024-04-23T23:54:21.127+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2024-04-23T23:54:16.535831+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-04-23T23:54:21.127+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2024-04-23T23:54:16.535831+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-23T23:54:21.155+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2024-04-23T23:54:16.535831+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-23T23:54:21.932+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2024-04-23T23:54:22.802+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: basic_etl_dag.transform_task manual__2024-04-23T23:54:16.535831+00:00 [queued]> on host codespaces-1d31e6[0m
[[34m2024-04-23T23:54:23.511+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2024-04-23T23:54:16.535831+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-23T23:54:23.514+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=transform_task, run_id=manual__2024-04-23T23:54:16.535831+00:00, map_index=-1, run_start_date=2024-04-23 23:54:22.872983+00:00, run_end_date=2024-04-23 23:54:23.080315+00:00, run_duration=0.207332, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=8, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-04-23 23:54:21.126845+00:00, queued_by_job_id=2, pid=10407[0m
[[34m2024-04-23T23:54:23.547+0000[0m] {[34mdagrun.py:[0m609} ERROR[0m - Marking run <DagRun basic_etl_dag @ 2024-04-23 23:54:16.535831+00:00: manual__2024-04-23T23:54:16.535831+00:00, state:running, queued_at: 2024-04-23 23:54:16.542552+00:00. externally triggered: True> failed[0m
[[34m2024-04-23T23:54:23.547+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=basic_etl_dag, execution_date=2024-04-23 23:54:16.535831+00:00, run_id=manual__2024-04-23T23:54:16.535831+00:00, run_start_date=2024-04-23 23:54:17.844490+00:00, run_end_date=2024-04-23 23:54:23.547297+00:00, run_duration=5.702807, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-04-23 23:54:16.535831+00:00, data_interval_end=2024-04-23 23:54:16.535831+00:00, dag_hash=268d066802c90e6ca656934998f34563[0m
[[34m2024-04-23T23:59:06.695+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-04-23T23:59:46.154+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.extract_task manual__2024-04-23T23:59:44.919370+00:00 [scheduled]>[0m
[[34m2024-04-23T23:59:46.154+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2024-04-23T23:59:46.155+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.extract_task manual__2024-04-23T23:59:44.919370+00:00 [scheduled]>[0m
[[34m2024-04-23T23:59:46.235+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2024-04-23T23:59:44.919370+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-04-23T23:59:46.235+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2024-04-23T23:59:44.919370+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-23T23:59:46.264+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2024-04-23T23:59:44.919370+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-23T23:59:47.079+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2024-04-23T23:59:48.010+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: basic_etl_dag.extract_task manual__2024-04-23T23:59:44.919370+00:00 [queued]> on host codespaces-1d31e6[0m
[[34m2024-04-23T23:59:49.497+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2024-04-23T23:59:44.919370+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-23T23:59:49.500+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=extract_task, run_id=manual__2024-04-23T23:59:44.919370+00:00, map_index=-1, run_start_date=2024-04-23 23:59:48.077029+00:00, run_end_date=2024-04-23 23:59:49.090031+00:00, run_duration=1.013002, state=success, executor_state=success, try_number=1, max_tries=0, job_id=3, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-04-23 23:59:46.155723+00:00, queued_by_job_id=2, pid=12452[0m
[[34m2024-04-23T23:59:49.599+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.transform_task manual__2024-04-23T23:59:44.919370+00:00 [scheduled]>[0m
[[34m2024-04-23T23:59:49.599+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2024-04-23T23:59:49.599+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.transform_task manual__2024-04-23T23:59:44.919370+00:00 [scheduled]>[0m
[[34m2024-04-23T23:59:49.601+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2024-04-23T23:59:44.919370+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-04-23T23:59:49.601+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2024-04-23T23:59:44.919370+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-23T23:59:49.630+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2024-04-23T23:59:44.919370+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-23T23:59:50.392+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2024-04-23T23:59:51.210+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: basic_etl_dag.transform_task manual__2024-04-23T23:59:44.919370+00:00 [queued]> on host codespaces-1d31e6[0m
[[34m2024-04-23T23:59:51.893+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2024-04-23T23:59:44.919370+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-23T23:59:51.896+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=transform_task, run_id=manual__2024-04-23T23:59:44.919370+00:00, map_index=-1, run_start_date=2024-04-23 23:59:51.277581+00:00, run_end_date=2024-04-23 23:59:51.481322+00:00, run_duration=0.203741, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-04-23 23:59:49.600180+00:00, queued_by_job_id=2, pid=12464[0m
[[34m2024-04-23T23:59:51.926+0000[0m] {[34mdagrun.py:[0m609} ERROR[0m - Marking run <DagRun basic_etl_dag @ 2024-04-23 23:59:44.919370+00:00: manual__2024-04-23T23:59:44.919370+00:00, state:running, queued_at: 2024-04-23 23:59:44.959673+00:00. externally triggered: True> failed[0m
[[34m2024-04-23T23:59:51.926+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=basic_etl_dag, execution_date=2024-04-23 23:59:44.919370+00:00, run_id=manual__2024-04-23T23:59:44.919370+00:00, run_start_date=2024-04-23 23:59:46.011004+00:00, run_end_date=2024-04-23 23:59:51.926884+00:00, run_duration=5.91588, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-04-23 23:59:44.919370+00:00, data_interval_end=2024-04-23 23:59:44.919370+00:00, dag_hash=e9ded275aab14d73c1f193742ff863f5[0m
[[34m2024-04-24T00:00:19.719+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.extract_task manual__2024-04-24T00:00:18.788109+00:00 [scheduled]>[0m
[[34m2024-04-24T00:00:19.719+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2024-04-24T00:00:19.719+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.extract_task manual__2024-04-24T00:00:18.788109+00:00 [scheduled]>[0m
[[34m2024-04-24T00:00:19.720+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2024-04-24T00:00:18.788109+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-04-24T00:00:19.721+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2024-04-24T00:00:18.788109+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-24T00:00:19.749+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2024-04-24T00:00:18.788109+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-24T00:00:20.560+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2024-04-24T00:00:21.385+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: basic_etl_dag.extract_task manual__2024-04-24T00:00:18.788109+00:00 [queued]> on host codespaces-1d31e6[0m
[[34m2024-04-24T00:00:22.645+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2024-04-24T00:00:18.788109+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-24T00:00:22.649+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=extract_task, run_id=manual__2024-04-24T00:00:18.788109+00:00, map_index=-1, run_start_date=2024-04-24 00:00:21.451057+00:00, run_end_date=2024-04-24 00:00:22.212592+00:00, run_duration=0.761535, state=success, executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-04-24 00:00:19.720115+00:00, queued_by_job_id=2, pid=12654[0m
[[34m2024-04-24T00:00:22.756+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.transform_task manual__2024-04-24T00:00:18.788109+00:00 [scheduled]>[0m
[[34m2024-04-24T00:00:22.756+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2024-04-24T00:00:22.757+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.transform_task manual__2024-04-24T00:00:18.788109+00:00 [scheduled]>[0m
[[34m2024-04-24T00:00:22.760+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2024-04-24T00:00:18.788109+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-04-24T00:00:22.760+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2024-04-24T00:00:18.788109+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-24T00:00:22.790+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2024-04-24T00:00:18.788109+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-24T00:00:23.571+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2024-04-24T00:00:24.390+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: basic_etl_dag.transform_task manual__2024-04-24T00:00:18.788109+00:00 [queued]> on host codespaces-1d31e6[0m
[[34m2024-04-24T00:00:25.065+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2024-04-24T00:00:18.788109+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-24T00:00:25.068+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=transform_task, run_id=manual__2024-04-24T00:00:18.788109+00:00, map_index=-1, run_start_date=2024-04-24 00:00:24.458538+00:00, run_end_date=2024-04-24 00:00:24.672246+00:00, run_duration=0.213708, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-04-24 00:00:22.757851+00:00, queued_by_job_id=2, pid=12666[0m
[[34m2024-04-24T00:00:25.100+0000[0m] {[34mdagrun.py:[0m609} ERROR[0m - Marking run <DagRun basic_etl_dag @ 2024-04-24 00:00:18.788109+00:00: manual__2024-04-24T00:00:18.788109+00:00, state:running, queued_at: 2024-04-24 00:00:18.792513+00:00. externally triggered: True> failed[0m
[[34m2024-04-24T00:00:25.101+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=basic_etl_dag, execution_date=2024-04-24 00:00:18.788109+00:00, run_id=manual__2024-04-24T00:00:18.788109+00:00, run_start_date=2024-04-24 00:00:19.647500+00:00, run_end_date=2024-04-24 00:00:25.101160+00:00, run_duration=5.45366, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-04-24 00:00:18.788109+00:00, data_interval_end=2024-04-24 00:00:18.788109+00:00, dag_hash=e9ded275aab14d73c1f193742ff863f5[0m
[[34m2024-04-24T00:04:06.792+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-04-24T00:08:23.751+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.extract_task manual__2024-04-24T00:08:22.579720+00:00 [scheduled]>[0m
[[34m2024-04-24T00:08:23.752+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2024-04-24T00:08:23.752+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.extract_task manual__2024-04-24T00:08:22.579720+00:00 [scheduled]>[0m
[[34m2024-04-24T00:08:23.754+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2024-04-24T00:08:22.579720+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-04-24T00:08:23.754+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2024-04-24T00:08:22.579720+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-24T00:08:23.784+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2024-04-24T00:08:22.579720+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-24T00:08:24.625+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2024-04-24T00:08:25.512+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: basic_etl_dag.extract_task manual__2024-04-24T00:08:22.579720+00:00 [queued]> on host codespaces-1d31e6[0m
[[34m2024-04-24T00:08:26.464+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2024-04-24T00:08:22.579720+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-24T00:08:26.468+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=extract_task, run_id=manual__2024-04-24T00:08:22.579720+00:00, map_index=-1, run_start_date=2024-04-24 00:08:25.578343+00:00, run_end_date=2024-04-24 00:08:26.024216+00:00, run_duration=0.445873, state=success, executor_state=success, try_number=1, max_tries=0, job_id=3, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-04-24 00:08:23.753005+00:00, queued_by_job_id=2, pid=15641[0m
[[34m2024-04-24T00:08:26.587+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.transform_task manual__2024-04-24T00:08:22.579720+00:00 [scheduled]>[0m
[[34m2024-04-24T00:08:26.587+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2024-04-24T00:08:26.587+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.transform_task manual__2024-04-24T00:08:22.579720+00:00 [scheduled]>[0m
[[34m2024-04-24T00:08:26.588+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2024-04-24T00:08:22.579720+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-04-24T00:08:26.589+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2024-04-24T00:08:22.579720+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-24T00:08:26.619+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2024-04-24T00:08:22.579720+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-24T00:08:27.399+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2024-04-24T00:08:28.291+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: basic_etl_dag.transform_task manual__2024-04-24T00:08:22.579720+00:00 [queued]> on host codespaces-1d31e6[0m
[[34m2024-04-24T00:08:29.009+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2024-04-24T00:08:22.579720+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-24T00:08:29.013+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=transform_task, run_id=manual__2024-04-24T00:08:22.579720+00:00, map_index=-1, run_start_date=2024-04-24 00:08:28.361462+00:00, run_end_date=2024-04-24 00:08:28.558420+00:00, run_duration=0.196958, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-04-24 00:08:26.587964+00:00, queued_by_job_id=2, pid=15665[0m
[[34m2024-04-24T00:08:29.043+0000[0m] {[34mdagrun.py:[0m609} ERROR[0m - Marking run <DagRun basic_etl_dag @ 2024-04-24 00:08:22.579720+00:00: manual__2024-04-24T00:08:22.579720+00:00, state:running, queued_at: 2024-04-24 00:08:22.619495+00:00. externally triggered: True> failed[0m
[[34m2024-04-24T00:08:29.043+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=basic_etl_dag, execution_date=2024-04-24 00:08:22.579720+00:00, run_id=manual__2024-04-24T00:08:22.579720+00:00, run_start_date=2024-04-24 00:08:23.641461+00:00, run_end_date=2024-04-24 00:08:29.043324+00:00, run_duration=5.401863, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-04-24 00:08:22.579720+00:00, data_interval_end=2024-04-24 00:08:22.579720+00:00, dag_hash=efae5970d2d3938bd90e8834f886cb47[0m
[[34m2024-04-24T00:09:06.821+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-04-24T00:11:57.722+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.extract_task manual__2024-04-24T00:11:56.988838+00:00 [scheduled]>[0m
[[34m2024-04-24T00:11:57.723+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2024-04-24T00:11:57.723+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.extract_task manual__2024-04-24T00:11:56.988838+00:00 [scheduled]>[0m
[[34m2024-04-24T00:11:57.828+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2024-04-24T00:11:56.988838+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-04-24T00:11:57.829+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2024-04-24T00:11:56.988838+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-24T00:11:57.857+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2024-04-24T00:11:56.988838+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-24T00:11:58.761+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2024-04-24T00:11:59.638+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: basic_etl_dag.extract_task manual__2024-04-24T00:11:56.988838+00:00 [queued]> on host codespaces-1d31e6[0m
[[34m2024-04-24T00:12:00.470+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2024-04-24T00:11:56.988838+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-24T00:12:00.473+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=extract_task, run_id=manual__2024-04-24T00:11:56.988838+00:00, map_index=-1, run_start_date=2024-04-24 00:11:59.704781+00:00, run_end_date=2024-04-24 00:11:59.975227+00:00, run_duration=0.270446, state=success, executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-04-24 00:11:57.723676+00:00, queued_by_job_id=2, pid=17014[0m
[[34m2024-04-24T00:12:00.545+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.transform_task manual__2024-04-24T00:11:56.988838+00:00 [scheduled]>[0m
[[34m2024-04-24T00:12:00.545+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2024-04-24T00:12:00.545+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.transform_task manual__2024-04-24T00:11:56.988838+00:00 [scheduled]>[0m
[[34m2024-04-24T00:12:00.547+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2024-04-24T00:11:56.988838+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-04-24T00:12:00.547+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2024-04-24T00:11:56.988838+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-24T00:12:00.575+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2024-04-24T00:11:56.988838+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-24T00:12:01.370+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2024-04-24T00:12:02.200+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: basic_etl_dag.transform_task manual__2024-04-24T00:11:56.988838+00:00 [queued]> on host codespaces-1d31e6[0m
[[34m2024-04-24T00:12:02.942+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2024-04-24T00:11:56.988838+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-24T00:12:02.945+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=transform_task, run_id=manual__2024-04-24T00:11:56.988838+00:00, map_index=-1, run_start_date=2024-04-24 00:12:02.273166+00:00, run_end_date=2024-04-24 00:12:02.507783+00:00, run_duration=0.234617, state=success, executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-04-24 00:12:00.546160+00:00, queued_by_job_id=2, pid=17026[0m
[[34m2024-04-24T00:12:03.047+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.load_task manual__2024-04-24T00:11:56.988838+00:00 [scheduled]>[0m
[[34m2024-04-24T00:12:03.047+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2024-04-24T00:12:03.047+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.load_task manual__2024-04-24T00:11:56.988838+00:00 [scheduled]>[0m
[[34m2024-04-24T00:12:03.048+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='load_task', run_id='manual__2024-04-24T00:11:56.988838+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-04-24T00:12:03.048+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'load_task', 'manual__2024-04-24T00:11:56.988838+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-24T00:12:03.078+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'load_task', 'manual__2024-04-24T00:11:56.988838+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-04-24T00:12:03.843+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2024-04-24T00:12:04.708+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: basic_etl_dag.load_task manual__2024-04-24T00:11:56.988838+00:00 [queued]> on host codespaces-1d31e6[0m
[[34m2024-04-24T00:12:05.442+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='basic_etl_dag', task_id='load_task', run_id='manual__2024-04-24T00:11:56.988838+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-24T00:12:05.445+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=load_task, run_id=manual__2024-04-24T00:11:56.988838+00:00, map_index=-1, run_start_date=2024-04-24 00:12:04.776310+00:00, run_end_date=2024-04-24 00:12:05.068045+00:00, run_duration=0.291735, state=success, executor_state=success, try_number=1, max_tries=0, job_id=7, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-04-24 00:12:03.047886+00:00, queued_by_job_id=2, pid=17050[0m
[[34m2024-04-24T00:12:05.616+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun basic_etl_dag @ 2024-04-24 00:11:56.988838+00:00: manual__2024-04-24T00:11:56.988838+00:00, state:running, queued_at: 2024-04-24 00:11:56.994013+00:00. externally triggered: True> successful[0m
[[34m2024-04-24T00:12:05.616+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=basic_etl_dag, execution_date=2024-04-24 00:11:56.988838+00:00, run_id=manual__2024-04-24T00:11:56.988838+00:00, run_start_date=2024-04-24 00:11:57.615809+00:00, run_end_date=2024-04-24 00:12:05.616471+00:00, run_duration=8.000662, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-04-24 00:11:56.988838+00:00, data_interval_end=2024-04-24 00:11:56.988838+00:00, dag_hash=efae5970d2d3938bd90e8834f886cb47[0m
[[34m2024-04-24T00:14:06.848+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-04-24T00:16:34.194+0000[0m] {[34mscheduler_job_runner.py:[0m247} INFO[0m - Exiting gracefully upon receiving signal 15[0m
[[34m2024-04-24T00:16:35.197+0000[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 6267. PIDs of all processes in the group: [6267][0m
[[34m2024-04-24T00:16:35.198+0000[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 6267[0m
[[34m2024-04-24T00:16:35.453+0000[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=6267, status='terminated', exitcode=0, started='23:44:06') (6267) terminated with exit code 0[0m
[[34m2024-04-24T00:16:35.456+0000[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 6267. PIDs of all processes in the group: [][0m
[[34m2024-04-24T00:16:35.456+0000[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 6267[0m
[[34m2024-04-24T00:16:35.457+0000[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending the signal Signals.SIGTERM to process 6267 as process group is missing.[0m
[[34m2024-04-24T00:16:35.457+0000[0m] {[34mscheduler_job_runner.py:[0m864} INFO[0m - Exited execute loop[0m
